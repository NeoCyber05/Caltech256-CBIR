{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T07:50:49.093628Z",
     "start_time": "2025-07-11T07:50:43.122004Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.datasets.caltech256 import Caltech256DataModule\n",
    "from src.featuring.rgb_histogram import RGBHistogram\n",
    "from src.storage.VectorDBStore import VectorDBStore\n",
    "from src.retrieval.KNN import KNNRetrieval\n",
    "from src.pipeline import CBIR\n",
    "from src.metrics import average_precision, recall, hit_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84c605e062ed5179",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T07:51:59.675756Z",
     "start_time": "2025-07-11T07:51:59.670234Z"
    }
   },
   "outputs": [],
   "source": [
    "os.makedirs('out', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c00ce3cb06df4f",
   "metadata": {},
   "source": [
    "# INDEXING"
   ]
  },
  {
   "cell_type": "code",
   "id": "1f730d283fee6a54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T08:36:59.919584Z",
     "start_time": "2025-07-11T08:28:37.518788Z"
    }
   },
   "source": [
    "TRAIN_SIZE = 24607  # Số lượng ảnh để index\n",
    "TEST_SIZE = 6000   # Số lượng ảnh để test\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Khởi tạo dataset\n",
    "import os\n",
    "root_path = os.path.abspath('data/caltech-256/256_ObjectCategories')\n",
    "print(f\"🔍 Dataset root: {root_path}\")\n",
    "print(f\"🔍 Exists: {os.path.exists(root_path)}\")\n",
    "data_module = Caltech256DataModule(batch_size=32, root=root_path)\n",
    "data_module.setup()\n",
    "train_loader = data_module.train_dataloader()\n",
    "test_loader = data_module.test_dataloader()\n",
    "\n",
    "# Hàm để test một metric\n",
    "def test_metric(metric_name):\n",
    "    print(f\"\\n🔬 Testing with {metric_name.upper()} metric...\")\n",
    "    \n",
    "    # Khởi tạo CBIR pipeline cho metric này\n",
    "    feature_extractor = RGBHistogram(n_bin=8, h_type=\"global\")\n",
    "    retrieval = KNNRetrieval(metric=metric_name)\n",
    "    storage = VectorDBStore(retrieval)\n",
    "    cbir = CBIR(feature_extractor, storage)\n",
    "    \n",
    "    # Indexing\n",
    "    print(f\"Indexing {TRAIN_SIZE} images...\")\n",
    "    start = time()\n",
    "    indexed = 0\n",
    "    \n",
    "    for images, labels, _ in tqdm(train_loader, desc=f\"Indexing ({metric_name})\"):\n",
    "        if indexed >= TRAIN_SIZE:\n",
    "            break\n",
    "    \n",
    "        if device.type == \"cuda\":\n",
    "            images = images.to(device)\n",
    "    \n",
    "        count = min(len(images), TRAIN_SIZE - indexed)\n",
    "        images = images[:count]\n",
    "        images = (images.cpu().numpy().transpose(0, 2, 3, 1) * 255).astype(np.uint8)\n",
    "    \n",
    "        cbir.add_images(images)\n",
    "        indexed += count\n",
    "    \n",
    "    indexing_time = time() - start\n",
    "    print(f\"Indexed {indexed} images in {indexing_time:.2f}s\")\n",
    "    \n",
    "    # Save model\n",
    "    model_path = f'out/caltech256_model_{metric_name}.pkl.gz'\n",
    "    with gzip.open(model_path, 'wb') as f:\n",
    "        pickle.dump(cbir, f)\n",
    "    \n",
    "    file_size = os.path.getsize(model_path) / 1024 / 1024\n",
    "    print(f\"💾 Model saved: {file_size:.2f} MB\")\n",
    "    \n",
    "    return cbir, indexing_time, file_size, indexed\n",
    "\n",
    "# Test cả 2 metrics\n",
    "results_comparison = {}\n",
    "models = {}\n",
    "\n",
    "for metric in [\"cosine\", \"euclidean\"]:\n",
    "    models[metric], indexing_time, file_size, indexed = test_metric(metric)\n",
    "    results_comparison[metric] = {\n",
    "        'indexing_time': indexing_time,\n",
    "        'file_size': file_size,\n",
    "        'indexed_images': indexed\n",
    "    }"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Dataset root: D:\\AI\\Food-CBIR\\src\\evaluation\\data\\caltech-256\\256_ObjectCategories\n",
      "🔍 Exists: True\n",
      "📂 Found 256 valid categories\n",
      "📊 Loaded 29780 total images from 256 classes\n",
      "📋 Train: 23824 images\n",
      "📂 Found 256 valid categories\n",
      "📊 Loaded 29780 total images from 256 classes\n",
      "📋 Test: 5956 images\n",
      "Train: 23824, Test: 5956\n",
      "\n",
      "🔬 Testing with COSINE metric...\n",
      "Indexing 24607 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing (cosine): 100%|██████████| 745/745 [05:21<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 23824 images in 321.99s\n",
      "💾 Model saved: 32.44 MB\n",
      "\n",
      "🔬 Testing with EUCLIDEAN metric...\n",
      "Indexing 24607 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing (euclidean): 100%|██████████| 745/745 [02:15<00:00,  5.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 23824 images in 135.78s\n",
      "💾 Model saved: 32.44 MB\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "5bbdf283e6cabd9f",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "id": "b0dc4951ac77839c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T09:04:52.428546Z",
     "start_time": "2025-07-11T08:54:48.834237Z"
    }
   },
   "source": [
    "# Hàm để evaluate một model\n",
    "def evaluate_model(cbir, metric_name):\n",
    "    print(f\"\\n📊 Evaluating {metric_name.upper()} model...\")\n",
    "    start = time()\n",
    "    results = []\n",
    "    ground_truth = []\n",
    "    tested = 0\n",
    "\n",
    "    # Get dataset targets for evaluation\n",
    "    dataset_targets = []\n",
    "    for images, labels, _ in train_loader:\n",
    "        if len(dataset_targets) >= indexed:\n",
    "            break\n",
    "        count = min(len(labels), indexed - len(dataset_targets))\n",
    "        dataset_targets.extend(labels[:count].numpy())\n",
    "    dataset_targets = np.array(dataset_targets)\n",
    "\n",
    "    # Query với k=100 để có đủ data cho tất cả metrics\n",
    "    MAX_K = 100\n",
    "\n",
    "    for images, labels, _ in tqdm(test_loader, desc=f\"Testing ({metric_name})\"):\n",
    "        if tested >= TEST_SIZE:\n",
    "            break\n",
    "\n",
    "        if device.type == \"cuda\":\n",
    "            images = images.to(device)\n",
    "\n",
    "        count = min(len(images), TEST_SIZE - tested)\n",
    "        images = images[:count]\n",
    "        labels = labels[:count]\n",
    "\n",
    "        images = (images.cpu().numpy().transpose(0, 2, 3, 1) * 255).astype(np.uint8)\n",
    "\n",
    "        for image in images:\n",
    "            if tested >= TEST_SIZE:\n",
    "                break\n",
    "            # Query với k=100 để tính được MAP@100, Recall@100\n",
    "            result = cbir.query_similar_images(image, k=MAX_K)\n",
    "            results.append(result)\n",
    "            tested += 1\n",
    "\n",
    "        ground_truth.extend(labels.numpy())\n",
    "\n",
    "    retrieval_time = time() - start\n",
    "    print(f\"Tested {tested} images in {retrieval_time:.2f}s\")\n",
    "\n",
    "    # Calculate metrics cho k=5, k=10, k=50\n",
    "    k_values = [5, 10, 50]\n",
    "    metrics_data = {}\n",
    "\n",
    "    print(f\"📈 Calculating metrics for k={k_values}...\")\n",
    "\n",
    "    for k in k_values:\n",
    "        map_k, recall_k, hit_k = [], [], []\n",
    "\n",
    "        for r, gt in zip(results, ground_truth):\n",
    "            # Lấy top-k results\n",
    "            top_k_results = r[:k]\n",
    "            indices = [item.index for item in top_k_results]\n",
    "            preds = np.take(dataset_targets, indices)\n",
    "            relevant = np.where(dataset_targets == gt)[0]\n",
    "\n",
    "            map_k.append(average_precision(preds.tolist(), [gt], k))\n",
    "            recall_k.append(recall(indices, relevant, k))\n",
    "            hit_k.append(hit_rate(preds.tolist(), [gt], k))\n",
    "\n",
    "        # Store metrics\n",
    "        metrics_data[f'mAP@{k}'] = np.mean(map_k)\n",
    "        metrics_data[f'Recall@{k}'] = np.mean(recall_k)\n",
    "        metrics_data[f'HitRate@{k}'] = np.mean(hit_k)\n",
    "\n",
    "        print(f\"   k={k}: mAP={np.mean(map_k):.4f}, Recall={np.mean(recall_k):.4f}, HR={np.mean(hit_k):.4f}\")\n",
    "\n",
    "    # Add timing metrics từ results_comparison\n",
    "    metrics_data['indexing_time'] = results_comparison[metric_name]['indexing_time']\n",
    "    metrics_data['retrieval_time'] = retrieval_time\n",
    "    metrics_data['file_size_mb'] = results_comparison[metric_name]['file_size']\n",
    "    metrics_data['indexed_images'] = results_comparison[metric_name]['indexed_images']\n",
    "    metrics_data['tested_images'] = tested\n",
    "    metrics_data['metric'] = metric_name\n",
    "\n",
    "    return metrics_data\n",
    "\n",
    "# Evaluate cả 2 models\n",
    "print(\"\\n🚀 Starting evaluation of both models...\")\n",
    "all_results = []\n",
    "\n",
    "for metric in [\"cosine\", \"euclidean\"]:\n",
    "    cbir = models[metric]\n",
    "    metrics = evaluate_model(cbir, metric)\n",
    "    all_results.append(metrics)\n",
    "    results_comparison[metric].update(metrics)\n",
    "\n",
    "# So sánh kết quả\n",
    "print(f\"\\n📊 COMPARISON RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Metric':<12} {'mAP@5':<8} {'mAP@10':<8} {'mAP@50':<8} {'Recall@10':<10} {'HR@5':<8}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for metric in [\"cosine\", \"euclidean\"]:\n",
    "    data = results_comparison[metric]\n",
    "    print(f\"{metric:<12} {data['mAP@5']:<8.4f} {data['mAP@10']:<8.4f} {data['mAP@50']:<8.4f} {data['Recall@10']:<10.4f} {data['HitRate@5']:<8.4f}\")\n",
    "\n",
    "# Tìm winner\n",
    "cosine_map5 = results_comparison['cosine']['mAP@5']\n",
    "euclidean_map5 = results_comparison['euclidean']['mAP@5']\n",
    "\n",
    "winner = \"cosine\" if cosine_map5 > euclidean_map5 else \"euclidean\"\n",
    "print(f\"\\n🏆 WINNER: {winner.upper()} (mAP@5: {results_comparison[winner]['mAP@5']:.4f})\")\n",
    "\n",
    "# Save detailed results\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df.to_csv('out/caltech256_metrics_comparison.csv', index=False)\n",
    "print(\"✅ Detailed results saved to out/caltech256_metrics_comparison.csv\")\n",
    "\n",
    "# Performance comparison\n",
    "print(f\"\\n⏱️  Performance Comparison:\")\n",
    "for metric in [\"cosine\", \"euclidean\"]:\n",
    "    data = results_comparison[metric]\n",
    "    print(f\"{metric.capitalize()}: Index={data['indexing_time']:.1f}s, Query={data['retrieval_time']:.1f}s, Size={data['file_size_mb']:.1f}MB\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.empty_cache()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Starting evaluation of both models...\n",
      "\n",
      "📊 Evaluating COSINE model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing (cosine): 100%|██████████| 187/187 [04:58<00:00,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tested 5956 images in 379.55s\n",
      "📈 Calculating metrics for k=[5, 10, 50]...\n",
      "   k=5: mAP=0.0054, Recall=0.0002, HR=0.0049\n",
      "   k=10: mAP=0.0072, Recall=0.0004, HR=0.0052\n",
      "   k=50: mAP=0.0107, Recall=0.0022, HR=0.0055\n",
      "\n",
      "📊 Evaluating EUCLIDEAN model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing (euclidean): 100%|██████████| 187/187 [02:22<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tested 5956 images in 221.13s\n",
      "📈 Calculating metrics for k=[5, 10, 50]...\n",
      "   k=5: mAP=0.0083, Recall=0.0003, HR=0.0068\n",
      "   k=10: mAP=0.0099, Recall=0.0005, HR=0.0061\n",
      "   k=50: mAP=0.0131, Recall=0.0022, HR=0.0056\n",
      "\n",
      "📊 COMPARISON RESULTS\n",
      "============================================================\n",
      "Metric       mAP@5    mAP@10   mAP@50   Recall@10  HR@5    \n",
      "------------------------------------------------------------\n",
      "cosine       0.0054   0.0072   0.0107   0.0004     0.0049  \n",
      "euclidean    0.0083   0.0099   0.0131   0.0005     0.0068  \n",
      "\n",
      "🏆 WINNER: EUCLIDEAN (mAP@5: 0.0083)\n",
      "✅ Detailed results saved to out/caltech256_metrics_comparison.csv\n",
      "\n",
      "⏱️  Performance Comparison:\n",
      "Cosine: Index=322.0s, Query=379.6s, Size=32.4MB\n",
      "Euclidean: Index=135.8s, Query=221.1s, Size=32.4MB\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d68a9e91f9651c47"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
