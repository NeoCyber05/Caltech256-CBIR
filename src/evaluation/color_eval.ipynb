{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T07:50:49.093628Z",
     "start_time": "2025-07-11T07:50:43.122004Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.datasets.caltech256 import Caltech256DataModule\n",
    "from src.featuring.rgb_histogram import RGBHistogram\n",
    "from src.storage.VectorDBStore import VectorDBStore\n",
    "from src.retrieval.KNN import KNNRetrieval\n",
    "from src.pipeline import CBIR\n",
    "from src.metrics import average_precision, recall, hit_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84c605e062ed5179",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T07:51:59.675756Z",
     "start_time": "2025-07-11T07:51:59.670234Z"
    }
   },
   "outputs": [],
   "source": [
    "os.makedirs('out', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c00ce3cb06df4f",
   "metadata": {},
   "source": [
    "# INDEXING"
   ]
  },
  {
   "cell_type": "code",
   "id": "1f730d283fee6a54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T08:36:59.919584Z",
     "start_time": "2025-07-11T08:28:37.518788Z"
    }
   },
   "source": [
    "TRAIN_SIZE = 24607  # Sá»‘ lÆ°á»£ng áº£nh Ä‘á»ƒ index\n",
    "TEST_SIZE = 6000   # Sá»‘ lÆ°á»£ng áº£nh Ä‘á»ƒ test\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Khá»Ÿi táº¡o dataset\n",
    "import os\n",
    "root_path = os.path.abspath('data/caltech-256/256_ObjectCategories')\n",
    "print(f\"ğŸ” Dataset root: {root_path}\")\n",
    "print(f\"ğŸ” Exists: {os.path.exists(root_path)}\")\n",
    "data_module = Caltech256DataModule(batch_size=32, root=root_path)\n",
    "data_module.setup()\n",
    "train_loader = data_module.train_dataloader()\n",
    "test_loader = data_module.test_dataloader()\n",
    "\n",
    "# HÃ m Ä‘á»ƒ test má»™t metric\n",
    "def test_metric(metric_name):\n",
    "    print(f\"\\nğŸ”¬ Testing with {metric_name.upper()} metric...\")\n",
    "    \n",
    "    # Khá»Ÿi táº¡o CBIR pipeline cho metric nÃ y\n",
    "    feature_extractor = RGBHistogram(n_bin=8, h_type=\"global\")\n",
    "    retrieval = KNNRetrieval(metric=metric_name)\n",
    "    storage = VectorDBStore(retrieval)\n",
    "    cbir = CBIR(feature_extractor, storage)\n",
    "    \n",
    "    # Indexing\n",
    "    print(f\"Indexing {TRAIN_SIZE} images...\")\n",
    "    start = time()\n",
    "    indexed = 0\n",
    "    \n",
    "    for images, labels, _ in tqdm(train_loader, desc=f\"Indexing ({metric_name})\"):\n",
    "        if indexed >= TRAIN_SIZE:\n",
    "            break\n",
    "    \n",
    "        if device.type == \"cuda\":\n",
    "            images = images.to(device)\n",
    "    \n",
    "        count = min(len(images), TRAIN_SIZE - indexed)\n",
    "        images = images[:count]\n",
    "        images = (images.cpu().numpy().transpose(0, 2, 3, 1) * 255).astype(np.uint8)\n",
    "    \n",
    "        cbir.add_images(images)\n",
    "        indexed += count\n",
    "    \n",
    "    indexing_time = time() - start\n",
    "    print(f\"Indexed {indexed} images in {indexing_time:.2f}s\")\n",
    "    \n",
    "    # Save model\n",
    "    model_path = f'out/caltech256_model_{metric_name}.pkl.gz'\n",
    "    with gzip.open(model_path, 'wb') as f:\n",
    "        pickle.dump(cbir, f)\n",
    "    \n",
    "    file_size = os.path.getsize(model_path) / 1024 / 1024\n",
    "    print(f\"ğŸ’¾ Model saved: {file_size:.2f} MB\")\n",
    "    \n",
    "    return cbir, indexing_time, file_size, indexed\n",
    "\n",
    "# Test cáº£ 2 metrics\n",
    "results_comparison = {}\n",
    "models = {}\n",
    "\n",
    "for metric in [\"cosine\", \"euclidean\"]:\n",
    "    models[metric], indexing_time, file_size, indexed = test_metric(metric)\n",
    "    results_comparison[metric] = {\n",
    "        'indexing_time': indexing_time,\n",
    "        'file_size': file_size,\n",
    "        'indexed_images': indexed\n",
    "    }"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Dataset root: D:\\AI\\Food-CBIR\\src\\evaluation\\data\\caltech-256\\256_ObjectCategories\n",
      "ğŸ” Exists: True\n",
      "ğŸ“‚ Found 256 valid categories\n",
      "ğŸ“Š Loaded 29780 total images from 256 classes\n",
      "ğŸ“‹ Train: 23824 images\n",
      "ğŸ“‚ Found 256 valid categories\n",
      "ğŸ“Š Loaded 29780 total images from 256 classes\n",
      "ğŸ“‹ Test: 5956 images\n",
      "Train: 23824, Test: 5956\n",
      "\n",
      "ğŸ”¬ Testing with COSINE metric...\n",
      "Indexing 24607 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing (cosine): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 745/745 [05:21<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 23824 images in 321.99s\n",
      "ğŸ’¾ Model saved: 32.44 MB\n",
      "\n",
      "ğŸ”¬ Testing with EUCLIDEAN metric...\n",
      "Indexing 24607 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing (euclidean): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 745/745 [02:15<00:00,  5.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 23824 images in 135.78s\n",
      "ğŸ’¾ Model saved: 32.44 MB\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "5bbdf283e6cabd9f",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "id": "b0dc4951ac77839c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T09:04:52.428546Z",
     "start_time": "2025-07-11T08:54:48.834237Z"
    }
   },
   "source": [
    "# HÃ m Ä‘á»ƒ evaluate má»™t model\n",
    "def evaluate_model(cbir, metric_name):\n",
    "    print(f\"\\nğŸ“Š Evaluating {metric_name.upper()} model...\")\n",
    "    start = time()\n",
    "    results = []\n",
    "    ground_truth = []\n",
    "    tested = 0\n",
    "\n",
    "    # Get dataset targets for evaluation\n",
    "    dataset_targets = []\n",
    "    for images, labels, _ in train_loader:\n",
    "        if len(dataset_targets) >= indexed:\n",
    "            break\n",
    "        count = min(len(labels), indexed - len(dataset_targets))\n",
    "        dataset_targets.extend(labels[:count].numpy())\n",
    "    dataset_targets = np.array(dataset_targets)\n",
    "\n",
    "    # Query vá»›i k=100 Ä‘á»ƒ cÃ³ Ä‘á»§ data cho táº¥t cáº£ metrics\n",
    "    MAX_K = 100\n",
    "\n",
    "    for images, labels, _ in tqdm(test_loader, desc=f\"Testing ({metric_name})\"):\n",
    "        if tested >= TEST_SIZE:\n",
    "            break\n",
    "\n",
    "        if device.type == \"cuda\":\n",
    "            images = images.to(device)\n",
    "\n",
    "        count = min(len(images), TEST_SIZE - tested)\n",
    "        images = images[:count]\n",
    "        labels = labels[:count]\n",
    "\n",
    "        images = (images.cpu().numpy().transpose(0, 2, 3, 1) * 255).astype(np.uint8)\n",
    "\n",
    "        for image in images:\n",
    "            if tested >= TEST_SIZE:\n",
    "                break\n",
    "            # Query vá»›i k=100 Ä‘á»ƒ tÃ­nh Ä‘Æ°á»£c MAP@100, Recall@100\n",
    "            result = cbir.query_similar_images(image, k=MAX_K)\n",
    "            results.append(result)\n",
    "            tested += 1\n",
    "\n",
    "        ground_truth.extend(labels.numpy())\n",
    "\n",
    "    retrieval_time = time() - start\n",
    "    print(f\"Tested {tested} images in {retrieval_time:.2f}s\")\n",
    "\n",
    "    # Calculate metrics cho k=5, k=10, k=50\n",
    "    k_values = [5, 10, 50]\n",
    "    metrics_data = {}\n",
    "\n",
    "    print(f\"ğŸ“ˆ Calculating metrics for k={k_values}...\")\n",
    "\n",
    "    for k in k_values:\n",
    "        map_k, recall_k, hit_k = [], [], []\n",
    "\n",
    "        for r, gt in zip(results, ground_truth):\n",
    "            # Láº¥y top-k results\n",
    "            top_k_results = r[:k]\n",
    "            indices = [item.index for item in top_k_results]\n",
    "            preds = np.take(dataset_targets, indices)\n",
    "            relevant = np.where(dataset_targets == gt)[0]\n",
    "\n",
    "            map_k.append(average_precision(preds.tolist(), [gt], k))\n",
    "            recall_k.append(recall(indices, relevant, k))\n",
    "            hit_k.append(hit_rate(preds.tolist(), [gt], k))\n",
    "\n",
    "        # Store metrics\n",
    "        metrics_data[f'mAP@{k}'] = np.mean(map_k)\n",
    "        metrics_data[f'Recall@{k}'] = np.mean(recall_k)\n",
    "        metrics_data[f'HitRate@{k}'] = np.mean(hit_k)\n",
    "\n",
    "        print(f\"   k={k}: mAP={np.mean(map_k):.4f}, Recall={np.mean(recall_k):.4f}, HR={np.mean(hit_k):.4f}\")\n",
    "\n",
    "    # Add timing metrics tá»« results_comparison\n",
    "    metrics_data['indexing_time'] = results_comparison[metric_name]['indexing_time']\n",
    "    metrics_data['retrieval_time'] = retrieval_time\n",
    "    metrics_data['file_size_mb'] = results_comparison[metric_name]['file_size']\n",
    "    metrics_data['indexed_images'] = results_comparison[metric_name]['indexed_images']\n",
    "    metrics_data['tested_images'] = tested\n",
    "    metrics_data['metric'] = metric_name\n",
    "\n",
    "    return metrics_data\n",
    "\n",
    "# Evaluate cáº£ 2 models\n",
    "print(\"\\nğŸš€ Starting evaluation of both models...\")\n",
    "all_results = []\n",
    "\n",
    "for metric in [\"cosine\", \"euclidean\"]:\n",
    "    cbir = models[metric]\n",
    "    metrics = evaluate_model(cbir, metric)\n",
    "    all_results.append(metrics)\n",
    "    results_comparison[metric].update(metrics)\n",
    "\n",
    "# So sÃ¡nh káº¿t quáº£\n",
    "print(f\"\\nğŸ“Š COMPARISON RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Metric':<12} {'mAP@5':<8} {'mAP@10':<8} {'mAP@50':<8} {'Recall@10':<10} {'HR@5':<8}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for metric in [\"cosine\", \"euclidean\"]:\n",
    "    data = results_comparison[metric]\n",
    "    print(f\"{metric:<12} {data['mAP@5']:<8.4f} {data['mAP@10']:<8.4f} {data['mAP@50']:<8.4f} {data['Recall@10']:<10.4f} {data['HitRate@5']:<8.4f}\")\n",
    "\n",
    "# TÃ¬m winner\n",
    "cosine_map5 = results_comparison['cosine']['mAP@5']\n",
    "euclidean_map5 = results_comparison['euclidean']['mAP@5']\n",
    "\n",
    "winner = \"cosine\" if cosine_map5 > euclidean_map5 else \"euclidean\"\n",
    "print(f\"\\nğŸ† WINNER: {winner.upper()} (mAP@5: {results_comparison[winner]['mAP@5']:.4f})\")\n",
    "\n",
    "# Save detailed results\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df.to_csv('out/caltech256_metrics_comparison.csv', index=False)\n",
    "print(\"âœ… Detailed results saved to out/caltech256_metrics_comparison.csv\")\n",
    "\n",
    "# Performance comparison\n",
    "print(f\"\\nâ±ï¸  Performance Comparison:\")\n",
    "for metric in [\"cosine\", \"euclidean\"]:\n",
    "    data = results_comparison[metric]\n",
    "    print(f\"{metric.capitalize()}: Index={data['indexing_time']:.1f}s, Query={data['retrieval_time']:.1f}s, Size={data['file_size_mb']:.1f}MB\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.empty_cache()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Starting evaluation of both models...\n",
      "\n",
      "ğŸ“Š Evaluating COSINE model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing (cosine): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 187/187 [04:58<00:00,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tested 5956 images in 379.55s\n",
      "ğŸ“ˆ Calculating metrics for k=[5, 10, 50]...\n",
      "   k=5: mAP=0.0054, Recall=0.0002, HR=0.0049\n",
      "   k=10: mAP=0.0072, Recall=0.0004, HR=0.0052\n",
      "   k=50: mAP=0.0107, Recall=0.0022, HR=0.0055\n",
      "\n",
      "ğŸ“Š Evaluating EUCLIDEAN model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing (euclidean): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 187/187 [02:22<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tested 5956 images in 221.13s\n",
      "ğŸ“ˆ Calculating metrics for k=[5, 10, 50]...\n",
      "   k=5: mAP=0.0083, Recall=0.0003, HR=0.0068\n",
      "   k=10: mAP=0.0099, Recall=0.0005, HR=0.0061\n",
      "   k=50: mAP=0.0131, Recall=0.0022, HR=0.0056\n",
      "\n",
      "ğŸ“Š COMPARISON RESULTS\n",
      "============================================================\n",
      "Metric       mAP@5    mAP@10   mAP@50   Recall@10  HR@5    \n",
      "------------------------------------------------------------\n",
      "cosine       0.0054   0.0072   0.0107   0.0004     0.0049  \n",
      "euclidean    0.0083   0.0099   0.0131   0.0005     0.0068  \n",
      "\n",
      "ğŸ† WINNER: EUCLIDEAN (mAP@5: 0.0083)\n",
      "âœ… Detailed results saved to out/caltech256_metrics_comparison.csv\n",
      "\n",
      "â±ï¸  Performance Comparison:\n",
      "Cosine: Index=322.0s, Query=379.6s, Size=32.4MB\n",
      "Euclidean: Index=135.8s, Query=221.1s, Size=32.4MB\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d68a9e91f9651c47"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
